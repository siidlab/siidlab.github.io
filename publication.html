<!DOCTYPE html><!--  This site was created in Webflow. https://webflow.com  --><!--  Last Published: Mon Mar 31 2025 20:34:06 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="67eaf29f50c4c51749a29760" data-wf-site="67741f07e9ba5833fdf9aea6">
<head>
  <meta charset="utf-8">
  <title>Publication</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/siid-lab.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">
</head>
<body class="body">
  <div id="w-node-_5f470aba-a59e-89b1-2f49-7caafac3a3f3-49a29760" class="w-layout-layout quick-stack wf-layout-layout">
    <div class="w-layout-cell">
      <section class="gallery-scroll"><img src="images/SIID-lab-logo-bw.jpg" loading="lazy" width="157" sizes="157px" alt="" srcset="images/SIID-lab-logo-bw-p-500.jpg 500w, images/SIID-lab-logo-bw-p-800.jpg 800w, images/SIID-lab-logo-bw-p-1080.jpg 1080w, images/SIID-lab-logo-bw-p-1600.jpg 1600w, images/SIID-lab-logo-bw.jpg 2000w">
        <div class="div-block"></div>
        <div class="container">
          <div class="gallery-wrapper">
            <div id="w-node-_8c4e39c4-db37-1d4e-c065-4fef61cbd1b0-49a29760" data-w-id="8c4e39c4-db37-1d4e-c065-4fef61cbd1b0" class="gallery-sticky">
              <a href="index.html" class="gallery-link">About<br></a>
              <a href="projects.html" class="gallery-link">Projects<br></a>
              <a href="#" class="gallery-link">Publications<br></a>
              <a href="#" class="gallery-link">Apply<br></a>
              <a href="#" class="gallery-link">Contact<br></a>
            </div>
          </div>
        </div>
      </section>
    </div>
    <div class="w-layout-cell cell">
      <div class="div-block-2"></div><img src="images/towards.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/towards-p-500.png 500w, images/towards-p-800.png 800w, images/towards-p-1080.png 1080w, images/towards.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"><strong class="bold-text"><br>Towards Personalized Navigation in XR: Design Recommendations to Accommodate Individual Differences<br><br></strong></span><span class="text-span-3"><strong>J. Lee</strong>, W. Stuerzlinger (2025). Towards Personalized Navigation in XR: Design Recommendations to Accommodate Individual Differences, IEEELocXR &#x27;25, 4 pages. 2025-03</span>.<br><br><span class="text-span-2">Abstract: Navigation interfaces in Extended Reality (XR) have traditionally targeted universal solutions that perform well for all users. However, research has shown that users exhibit distinct preferences and performance patterns when using different navigation techniques. This position paper argues for the necessity of and strategies for designing personalized navigation interfaces that accommodate individual differences in spatial abilities, navigation strategies, and individual needs. Drawing from empirical findings from previous work investigating locomotion and wayfinding techniques and re- search in spatial cognition and navigation, we demonstrate how different user groups respond uniquely to navigation interface components. Based on these insights, we propose design recommendations for developing adaptive navigation interfaces that cater to individual user characteristics while maintaining usability. Further- more, we discuss opportunities for standardization in user assess- ment, interface adaptation, and inclusive design. This approach could lead to more inclusive and effective navigation solutions for XR environments.</span></p>
      <div class="div-block-2"></div><img src="images/scaling.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/scaling-p-500.png 500w, images/scaling-p-800.png 800w, images/scaling-p-1080.png 1080w, images/scaling.png 1280w" alt="" class="image-2">
      <p class="paragraph paragraph-publications"><span class="text-span"></span>
        <a href="https://vvise.iat.sfu.ca/user/data/papers/scalingtechniques.pdf" target="_blank"><span class="text-span"><strong class="bold-text"><br>Scaling Technique for Exocentric Navigation Interface in Multiscale Virtual Environments<br><br>‍</strong></span></a><span class="text-span"></span><strong>J. Lee</strong>, W. Stuerzlinger (2025). Scaling Technique for Exocentric Navigation in Multiscale Virtual Environments, IEEE TVCG &#x27;25, 9 pages. 2025-03<br><br><span class="text-span-2">Abstract: Navigating multiscale virtual environments necessitates an interaction method to travel across different levels of scale (LoS). Prior research has studied various techniques that enable users to seamlessly adjust their scale to navigate between different LoS based on specific user contexts. We introduce a scroll-based scale control method optimized for exocentric navigation, targeted at scenarios where speed and accuracy in continuous scaling are crucial. We pinpoint the challenges of scale control in settings with multiple LoS and evaluate how distinct designs of scaling techniques influence navigation performance and usability. Through a user study, we investigated two pivotal elements of a scaling technique: the input method and the scaling center. Our findings indicate that our scroll-based input method significantly reduces task completion time and error rate and enhances efficiency compared to the most frequently used bi-manual method. Moreover, we found that the choice of scaling center affects the ease of use of the scaling method, especially when paired with specific input methods.</span>
      </p>
      <div class="div-block-2"></div><img src="images/viewpoint.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/viewpoint-p-500.png 500w, images/viewpoint-p-800.png 800w, images/viewpoint-p-1080.png 1080w, images/viewpoint.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications">
        <a href="https://drive.google.com/file/d/1NUNO9G0bZI5IeuiLVW_1CFKes39uuRco/view?usp=sharing" target="_blank"><span class="text-span"><strong class="bold-text"><br>Designing Viewpoint Transition Techniques in Multiscale Virtual Environments</strong></span></a>J. <br><br><strong>J. Lee</strong>, P. Asente, W. Stuerzlinger (2023). Designing Viewpoint Transition Techinques in Multiscale Virtual Environments, IEEE VR &#x27;23, 9 pages. 2023-03<br><br><span class="text-span-2">Abstract: Viewpoint transitions have been shown to improve users&#x27; spatial orientation and help them build a cognitive map when they are navigating an unfamiliar virtual environment. Previous work has investigated transitions in single-scale virtual environments, focusing on trajectories and continuity. We extend this work with an in-depth investigation of transition techniques in multiscale virtual environments (MVEs). We identify challenges in navigating MVEs with nested structures and assess how different transition techniques affect spatial understanding and usability. Through two user studies, we investigated transition trajectories, interactive control of transition movement, and speed modulation in a nested MVE. We show that some types of viewpoint transitions enhance users&#x27; spatial awareness and confidence in their spatial orientation and reduce the need to revisit a target point of interest multiple times.</span>
      </p>
      <div class="div-block-2"></div><img src="images/zoom-in.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/zoom-in-p-500.png 500w, images/zoom-in-p-800.png 800w, images/zoom-in-p-1080.png 1080w, images/zoom-in.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://vvise.iat.sfu.ca/pubs/lee2022multiscalenavposter" target="_blank"><span class="text-span"><strong class="bold-text"><br>A Comparison of Zoom-In Transition Methods for Multiscale VR<br>‍<br>‍</strong></span></a><span class="text-span"></span><strong>J. Lee</strong>, P. Asente, W. Stuerzlinger (2022). A Comparison of Zoom-In Transition Methods for Multiscale VR, ACM SIGGRAPH &#x27;22, 2 pages. Poster. 2 pages.  2022-08<br><br><span class="text-span-2">Abstract: When navigating within an unfamiliar virtual environment in VR, transitions between pre-defined viewpoints are known to facilitate spatial awareness of a user. Previously, different viewpoint transition techniques had been investigated, but mainly for single-scale environments. We present a comparative study of zoom-in transition techniques, where the viewpoint of a user is being smoothly transitioned from a large level of scale (LoS) to a smaller LoS in a multiscale virtual environment (MVE) with a nested structure. We identify that orbiting first before zooming in is preferred over other alternatives when transitioning to a viewpoint at a small LoS.</span>
      </p>
      <div class="div-block-2"></div><img src="images/loco.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/loco-p-500.png 500w, images/loco-p-800.png 800w, images/loco-p-1080.png 1080w, images/loco.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://doi.org/10.1145/3385956.3418961" target="_blank"><span class="text-span"><strong class="bold-text"><br>Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments<br>‍</strong></span></a><span class="text-span"></span><strong>J. Lee</strong>, P. Asente, B. Kim, Y. Kim, W. Stuerzlinger (2020). Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments, ACM VRST &#x27;20, 10 pages.    2018-04<br><br><span class="text-span-2">Abstract: Virtual environments with a wide range of scales are becoming commonplace in Virtual Reality applications. Methods to control locomotion parameters can help users explore such environments more easily. For multi-scale virtual environments, point-and-teleport locomotion with a well-designed distance control method can enable mid-air teleportation, which makes it competitive to flying interfaces. Yet, automatic distance control for point-and-teleport has not been studied in such environments. We present a new method to automatically control the distance for point-and-teleport. In our first user study, we used a solar system environment to compare three methods: automatic distance control for point-and-teleport, manual distance control for point-and-teleport, and automatic speed control for flying. Results showed that automatic control significantly reduces overshoot compared with manual control for pointand-teleport, but the discontinuous nature of teleportation made users prefer flying with automatic speed control. We conducted a second study to compare automatic-speed-controlled flying and two versions of our teleportation method with automatic distance control, one incorporating optical flow cues. We found that pointand-teleport with optical flow cues and automatic distance control was more accurate than flying with automatic speed control, and both were equally preferred to point-and-teleport without the cues.</span>
      </p>
      <div class="div-block-2"></div><img src="images/movingtarget.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/movingtarget-p-500.png 500w, images/movingtarget-p-800.png 800w, images/movingtarget-p-1080.png 1080w, images/movingtarget.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://dl.acm.org/doi/10.1145/3173574.3173804" target="_blank"><span class="text-span"><strong class="bold-text"><br>Moving Target Selection: A Cue Integration Model<br><br>‍</strong></span></a><span class="text-span"></span>B. Lee, S. Kim, A. Oulasvirta, <strong>J. Lee</strong>, E. Park (2018). Moving Target Selection: A Cue Integration Model, ACM CHI &#x27;18, 10 pages.  2018-05<br><br><span class="text-span-2">Abstract: This paper investigates a common task requiring temporal precision: the selection of a rapidly moving target on display by invoking an input event when it is within some selection window. Previous work has explored the relationship between accuracy and precision in this task, but the role of visual cues available to users has remained unexplained. To expand modeling of timing performance to multimodal settings, common in gaming and music, our model builds on the principle of probabilistic cue integration. Maximum likelihood estimation (MLE) is used to model how different types of cues are integrated into a reliable estimate of the temporal task. The model deals with temporal structure (repetition, rhythm) and the perceivable movement of the target on display. It accurately predicts error rate in a range of realistic tasks. Applications include the optimization of difficulty in game-level design.</span>
      </p>
      <div class="div-block-2"></div><img src="images/reflector.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/reflector-p-500.png 500w, images/reflector-p-800.png 800w, images/reflector-p-1080.png 1080w, images/reflector.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications">
        <a href="https://dl.acm.org/doi/10.1145/3173574.3173804" target="_blank"><span class="text-span"><strong class="bold-text"><br>Reflector: Distance-Independent, Private Pointing on a Reflective Screen<br>‍<br>‍</strong></span></a><strong>J. Lee</strong>, S. Kim, M. Fukumoto, B. Lee (2017). Reflector: Distance-Independent, Private Pointing on a Reflective Screen, ACM UIST &#x27;17, 10 pages.  2017-10<br><br><span class="text-span-2">Abstract: Reﬂector is a novel direct pointing method that utilizes hidden design space on reﬂective screens. By aligning a part of the user’s onscreen reﬂection with objects rendered on the screen, Reﬂector enables (1) distance-independent and (2) private pointing on commodity screens. Reﬂector can be implemented easily in both desktop and mobile conditions through a single camera installed at the edge of the screen. Reﬂector’s pointing performance was compared to today’s major direct input devices: eye trackers and touchscreens. We demonstrate that Reﬂector allows the user to point more reliably, regardless of distance from the screen, compared to an eye tracker. Further, due to the private nature of an onscreen reﬂection, Reﬂector shows a shoulder surﬁng success rate 20 times lower than that of touchscreens for the task of entering a 4-digit PIN.</span>
      </p>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=67741f07e9ba5833fdf9aea6" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
</body>
</html>