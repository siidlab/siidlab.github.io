<!DOCTYPE html><!--  This site was created in Webflow. https://webflow.com  --><!--  Last Published: Sun Apr 27 2025 21:17:16 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="67eaf29f50c4c51749a29760" data-wf-site="67741f07e9ba5833fdf9aea6">
<head>
  <meta charset="utf-8">
  <title>Publication</title>
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <meta content="Webflow" name="generator">
  <link href="css/normalize.css" rel="stylesheet" type="text/css">
  <link href="css/webflow.css" rel="stylesheet" type="text/css">
  <link href="css/siid-lab.webflow.css" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic"]  }});</script>
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="images/favicon.ico" rel="shortcut icon" type="image/x-icon">
  <link href="images/webclip.png" rel="apple-touch-icon">

  <script>
  // Function to detect mobile device
  function isMobileDevice() {
    return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
  }

  // Function to set desktop view for mobile devices
  function forceDesktopView() {
    // Only apply desktop view settings if on a mobile device
    if (isMobileDevice()) {
      // Get viewport meta tag
      var viewport = document.querySelector('meta[name="viewport"]');
      
      // If viewport meta tag exists, modify it
      if (viewport) {
        // Using a lower initial-scale value (0.3) to fit more content in the viewport
        viewport.setAttribute('content', 'width=1280, initial-scale=0.3');
      } 
      // If viewport tag doesn't exist, create one
      else {
        var newViewport = document.createElement('meta');
        newViewport.setAttribute('name', 'viewport');
        newViewport.setAttribute('content', 'width=1280, initial-scale=0.3');
        document.head.appendChild(newViewport);
      }
    }
  }

  // When the page loads, force desktop view
  window.addEventListener('DOMContentLoaded', forceDesktopView);
  </script>
</head>
<body class="body">
  <div id="w-node-_5f470aba-a59e-89b1-2f49-7caafac3a3f3-49a29760" class="w-layout-layout quick-stack wf-layout-layout">
    <div class="w-layout-cell">
      <section class="gallery-scroll"><img src="images/SIID-lab-logo-bw.jpg" loading="lazy" width="157" sizes="157px" alt="" srcset="images/SIID-lab-logo-bw-p-500.jpg 500w, images/SIID-lab-logo-bw-p-800.jpg 800w, images/SIID-lab-logo-bw-p-1080.jpg 1080w, images/SIID-lab-logo-bw-p-1600.jpg 1600w, images/SIID-lab-logo-bw.jpg 2000w">
        <div class="div-block"></div>
        <div class="container">
          <div class="gallery-wrapper">
            <div id="w-node-_8c4e39c4-db37-1d4e-c065-4fef61cbd1b0-49a29760" data-w-id="8c4e39c4-db37-1d4e-c065-4fef61cbd1b0" class="gallery-sticky">
              <a href="index.html" class="gallery-link">About<br></a>
              <a href="members.html" class="gallery-link">Members<br></a>
              <a href="projects.html" class="gallery-link">Projects<br></a>
              <a href="publication.html" class="gallery-link">Publications<br></a>
              <a href="apply.html" class="gallery-link">Apply<br></a>
              <a href="mailto:jongin@tamu.edu" class="gallery-link">Contact<br></a>
            </div>
          </div>
        </div>
      </section>
    </div>
    <div class="w-layout-cell cell">


      <div class="div-block-2"></div><img src="images/conceptualFramework.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/conceptualFramework.png 500w, images/conceptualFramework-p-800.png 800w, images/conceptualFramework-p-1080.png 1080w, images/conceptualFramework.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://github.com/siidlab/siidlab.github.io/blob/main/papers/beyondOneSizeFitsAll.pdf"><span class="text-span"><strong class="bold-text"><br>A Conceptual Framework for Personalizing XR Navigation<br></strong></span></a>
        <br><span class="text-span"></span><span class="text-span-3">J. Rhim and <strong>J. Lee</strong> (2026). Beyond One-Size-Fits-All: A Conceptual Framework for Personalizing XR Navigation Based on Individual Capacity and Context, IEEE VR Workshop, 4 pages. 2026-03</span>.<br><br><span class="text-span-2">We introduce a framework challenging the one-size-fits-all approach to XR navigation design by addressing individual differences in spatial cognition. We identify three key components for effective personalization: incorporating user characteristics and individual capacity, considering varied application domains, and implementing personalized adaptive navigation interfaces.</span>
      </p>


      <div class="div-block-2"></div><img src="images/speechInput.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/speechInput.png 500w, images/speechInput-p-800.png 800w, images/speechInput-p-1080.png 1080w, images/speechInput.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://github.com/siidlab/siidlab.github.io/blob/main/papers/speech-basedScaleControl.pdf"><span class="text-span"><strong class="bold-text"><br>LLM-powered Speech-based Interaction for Multiscale Medical AR Applications<br></strong></span></a><span class="text-span"></span>
        <br><span class="text-span-3">V. Agarwal, R. Kopper, and <strong>J. Lee</strong>. More Than Speed: User Agency and Social Comfort in Speech-based Interaction for Multiscale Medical AR Applications, IEEE VR Workshop, 4 pages. 2026-03</span>.<br><br><span class="text-span-2">We present a natural speech-based technique for viewpoint control in medical AR environments and evaluate it through a mixed-methods study. Our findings suggest that AI-assisted navigation was favored for its autonomy and sense of control, with psychological benefits like user agency and social comfort being more important than absolute speed.</span>
      </p>


      <div class="div-block-2"></div><img src="images/wreck_deepwater.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/wreck_deepwater-p-500.png 500w, images/wreck_deepwater-p-800.png 800w, images/wreck_deepwater-p-1080.png 1080w, images/wreck_deepwater.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://github.com/siidlab/siidlab.github.io/blob/main/papers/Thalossophobia_simulator.pdf"><span class="text-span"><strong class="bold-text"><br>Audio-Visual Cue Design for Stress-Response Training in Underwater VR Environments<br></strong></span></a><span class="text-span"></span>
        <br><span class="text-span-3"><strong>J. Lee</strong>, B. Kim, B. P. Casey, A. Castro-Rosas, D. Turner, D. Nunez (2026). Project Deep Dive: Investigating Audio-Visual Cue Design for Stress-Response Training in Underwater VR Environments, IEEE VR Workshop, 4 pages. 2025-03</span>.<br><br><span class="text-span-2">We developed a VR training prototype that investigates how audio-visual stimuli evoke stress responses in underwater environments to help trainees identify fear thresholds and practice emotional regulation. Our preliminary findings from a pilot study suggest that spatial audio, particularly sounds implying unseen threats, may be more effective than visual effects in inducing immersion and discomfort.</span>
      </p>


      <div class="div-block-2"></div><img src="images/towards.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/towards-p-500.png 500w, images/towards-p-800.png 800w, images/towards-p-1080.png 1080w, images/towards.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://vvise.iat.sfu.ca/user/data/papers/personalizednav.pdf"><span class="text-span"><strong class="bold-text"><br>Design Recommendations for XR navigation interface to Accommodate Individual Differences<br></strong></span></a><span class="text-span"></span>
        <br><span class="text-span-3"><strong>J. Lee</strong>, W. Stuerzlinger (2025). Towards Personalized Navigation in XR: Design Recommendations to Accommodate Individual Differences, IEEELocXR &#x27;25, 4 pages. 2025-03</span>.<br><br><span class="text-span-2">We argue for personalized navigation interfaces that accommodate individual differences in spatial abilities and navigation strategies rather than universal solutions. Drawing from empirical findings, we propose design recommendations for developing adaptive navigation interfaces and discuss opportunities for standardization in user assessment and inclusive design.</span>
      </p>
      <div class="div-block-2"></div><img src="images/scaling.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/scaling-p-500.png 500w, images/scaling-p-800.png 800w, images/scaling-p-1080.png 1080w, images/scaling.png 1280w" alt="" class="image-2">
      <p class="paragraph paragraph-publications"><span class="text-span"></span>
        <a href="https://vvise.iat.sfu.ca/user/data/papers/scalingtechniques.pdf" target="_blank"><span class="text-span"><strong class="bold-text"><br>Scaling Technique for Navigating Multiscale Virtual Environments<br><br>‍</strong></span></a><span class="text-span"></span><strong>J. Lee</strong>, W. Stuerzlinger (2025). Scaling Technique for Exocentric Navigation in Multiscale Virtual Environments, IEEE TVCG &#x27;25, 9 pages. 2025-03<br><br><span class="text-span-2"> We introduce a scroll-based scale control method optimized for exocentric navigation in multiscale environments where speed and accuracy are crucial. Our user study findings indicate that the scroll-based input method significantly reduces task completion time and error rate compared to bi-manual methods.</span>
      </p>
      <div class="div-block-2"></div><img src="images/viewpoint.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/viewpoint-p-500.png 500w, images/viewpoint-p-800.png 800w, images/viewpoint-p-1080.png 1080w, images/viewpoint.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications">
        <a href="https://drive.google.com/file/d/1NUNO9G0bZI5IeuiLVW_1CFKes39uuRco/view?usp=sharing" target="_blank"><span class="text-span"><strong class="bold-text"><br>Viewpoint Transition Techniques in Multiscale Virtual Environments</strong></span></a>J. <br><br><strong>J. Lee</strong>, P. Asente, W. Stuerzlinger (2023). Designing Viewpoint Transition Techinques in Multiscale Virtual Environments, IEEE VR &#x27;23, 9 pages. 2023-03<br><br><span class="text-span-2">We extend viewpoint transition research to multiscale virtual environments with nested structures through two user studies investigating transition trajectories, interactive control, and speed modulation. Our results show that certain viewpoint transitions enhance spatial awareness and confidence while reducing the need to revisit target points of interest.</span>
      </p>
      <div class="div-block-2"></div><img src="images/zoom-in.png" loading="lazy" sizes="(max-width: 1280px) 100vw, 1280px" srcset="images/zoom-in-p-500.png 500w, images/zoom-in-p-800.png 800w, images/zoom-in-p-1080.png 1080w, images/zoom-in.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://vvise.iat.sfu.ca/pubs/lee2022multiscalenavposter" target="_blank"><span class="text-span"><strong class="bold-text"><br>Zoom-In Transition in Multiscale VR<br>‍<br>‍</strong></span></a><span class="text-span"></span><strong>J. Lee</strong>, P. Asente, W. Stuerzlinger (2022). A Comparison of Zoom-In Transition Methods for Multiscale VR, ACM SIGGRAPH &#x27;22, 2 pages. Poster. 2 pages.  2022-08<br><br><span class="text-span-2">We present a comparative study examining zoom-in transition techniques where the viewpoint transitions from large to small levels of scale in multiscale virtual environments with nested structures. We identify that orbiting first before zooming in is preferred over other alternatives when transitioning to viewpoints at smaller scales.</span>
      </p>
      <div class="div-block-2"></div><img src="images/loco.png" loading="lazy" sizes="100vw" srcset="images/loco-p-500.png 500w, images/loco-p-800.png 800w, images/loco-p-1080.png 1080w, images/loco.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://doi.org/10.1145/3385956.3418961" target="_blank"><span class="text-span"><strong class="bold-text"><br>Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments<br>‍</strong></span></a>
        <br><span class="text-span"></span><strong>J. Lee</strong>, P. Asente, B. Kim, Y. Kim, W. Stuerzlinger (2020). Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments, ACM VRST &#x27;20, 10 pages.    2018-04<br><br><span class="text-span-2">We present a new method for automatically controlling distance in point-and-teleport locomotion for multi-scale environments. Our two user studies found that automatic distance control reduces overshoot compared to manual control, and point-and-teleport with optical flow cues and automatic distance control was more accurate than flying with automatic speed control.</span>
      </p>
      <div class="div-block-2"></div><img src="images/movingtarget.png" loading="lazy" sizes="100vw" srcset="images/movingtarget-p-500.png 500w, images/movingtarget-p-800.png 800w, images/movingtarget-p-1080.png 1080w, images/movingtarget.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications"><span class="text-span"></span>
        <a href="https://dl.acm.org/doi/10.1145/3173574.3173804" target="_blank"><span class="text-span"><strong class="bold-text"><br>A Cue Integration Model for moving target selection<br><br>‍</strong></span></a><span class="text-span"></span>B. Lee, S. Kim, A. Oulasvirta, <strong>J. Lee</strong>, E. Park (2018). Moving Target Selection: A Cue Integration Model, ACM CHI &#x27;18, 10 pages.  2018-05<br><br><span class="text-span-2">We investigate the selection of rapidly moving targets on display and build a model based on probabilistic cue integration using maximum likelihood estimation. Our model accurately predicts error rates across realistic tasks by dealing with temporal structure and perceivable movement, with applications in optimizing difficulty in game-level design.</span>
      </p>
      <div class="div-block-2"></div><img src="images/reflector.png" loading="lazy" sizes="100vw" srcset="images/reflector-p-500.png 500w, images/reflector-p-800.png 800w, images/reflector-p-1080.png 1080w, images/reflector.png 1280w" alt="" class="image-2">
      <p class="paragraph-publications">
        <a href="https://dl.acm.org/doi/10.1145/3126594.3126665" target="_blank"><span class="text-span"><strong class="bold-text"><br>Pointing Technique on a Reflective Screen<br>‍<br>‍</strong></span></a><strong>J. Lee</strong>, S. Kim, M. Fukumoto, B. Lee (2017). Reflector: Distance-Independent, Private Pointing on a Reflective Screen, ACM UIST &#x27;17, 10 pages.  2017-10<br><br><span class="text-span-2">We developed Reflector, a novel direct pointing method that uses onscreen reflections to enable distance-independent and private pointing on commodity screens. Our comparison studies show that Reflector allows more reliable pointing regardless of distance compared to eye trackers and demonstrates significantly better privacy protection than touchscreens for PIN entry tasks.</span>
      </p>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=67741f07e9ba5833fdf9aea6" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="js/webflow.js" type="text/javascript"></script>
</body>
</html>